{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964   @user #white #supremacists want everyone to s...\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966  is the hp and the cursed child book up for res...\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most basic requirement in NLP analysis is to count the number of words in each tweet. The idea behind this is that the negative sentiments contain a lesser amount of words than the positive ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_words(df):\n",
    "    df['word_count'] = df['tweet'].apply(lambda x : len(str(x).split(\" \")))\n",
    "    print(df[['tweet','word_count']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  word_count\n",
      "0   @user when a father is dysfunctional and is s...          21\n",
      "1  @user @user thanks for #lyft credit i can't us...          22\n",
      "2                                bihday your majesty           5\n",
      "3  #model   i love u take with u all the time in ...          17\n",
      "4             factsguide: society now    #motivation           8\n"
     ]
    }
   ],
   "source": [
    "num_of_words(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  word_count\n",
      "0  #studiolife #aislife #requires #passion #dedic...          12\n",
      "1   @user #white #supremacists want everyone to s...          20\n",
      "2  safe ways to heal your #acne!!    #altwaystohe...          15\n",
      "3  is the hp and the cursed child book up for res...          24\n",
      "4    3rd #bihday to my amazing, hilarious #nephew...          18\n"
     ]
    }
   ],
   "source": [
    "num_of_words(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, number of words and number of characters are important. But, there is another feature which is also important is average word length of each tweet. This feature can help us to improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    words = sentence.split()    \n",
    "    return (sum(len(word) for word in words)/len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_length(df):\n",
    "    df['avg_word'] = df['tweet'].apply(lambda x: avg_word(x))\n",
    "    print(df[['tweet','avg_word']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  avg_word\n",
      "0   @user when a father is dysfunctional and is s...  4.555556\n",
      "1  @user @user thanks for #lyft credit i can't us...  5.315789\n",
      "2                                bihday your majesty  5.666667\n",
      "3  #model   i love u take with u all the time in ...  4.928571\n",
      "4             factsguide: society now    #motivation  8.000000\n"
     ]
    }
   ],
   "source": [
    "avg_word_length(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  avg_word\n",
      "0  #studiolife #aislife #requires #passion #dedic...  8.777778\n",
      "1   @user #white #supremacists want everyone to s...  5.125000\n",
      "2  safe ways to heal your #acne!!    #altwaystohe...  6.333333\n",
      "3  is the hp and the cursed child book up for res...  5.409091\n",
      "4    3rd #bihday to my amazing, hilarious #nephew...  5.066667\n"
     ]
    }
   ],
   "source": [
    "avg_word_length(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, while solving any NLP problem, the first thing we do is to remove the stopwords. But, what are stop words?\n",
    "Stop Words : A stop word is a commonly used word such as the, a, an, in which are filtered out before or after processing of natural language data (text). Sometimes calculating the number of stopwords can also give us some extra information which we might have been losing before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop= stopwords.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words(df):\n",
    "    df['stopwords'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "    print(df[['tweet','stopwords']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  stopwords\n",
      "0   @user when a father is dysfunctional and is s...         10\n",
      "1  @user @user thanks for #lyft credit i can't us...          5\n",
      "2                                bihday your majesty          1\n",
      "3  #model   i love u take with u all the time in ...          5\n",
      "4             factsguide: society now    #motivation          1\n"
     ]
    }
   ],
   "source": [
    "stop_words(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  stopwords\n",
      "0  #studiolife #aislife #requires #passion #dedic...          1\n",
      "1   @user #white #supremacists want everyone to s...          4\n",
      "2  safe ways to heal your #acne!!    #altwaystohe...          2\n",
      "3  is the hp and the cursed child book up for res...          8\n",
      "4    3rd #bihday to my amazing, hilarious #nephew...          4\n"
     ]
    }
   ],
   "source": [
    "stop_words(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numbers and special chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more interesting feature which we can extract from a tweet is to calculate the number of hashtags in it. It also helps in extracting extra information from our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_tags(df):\n",
    "    df['hashtags'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "    print(df[['tweet','hashtags']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  hashtags\n",
      "0   @user when a father is dysfunctional and is s...         1\n",
      "1  @user @user thanks for #lyft credit i can't us...         3\n",
      "2                                bihday your majesty         0\n",
      "3  #model   i love u take with u all the time in ...         1\n",
      "4             factsguide: society now    #motivation         1\n"
     ]
    }
   ],
   "source": [
    "hash_tags(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  hashtags\n",
      "0  #studiolife #aislife #requires #passion #dedic...         7\n",
      "1   @user #white #supremacists want everyone to s...         4\n",
      "2  safe ways to heal your #acne!!    #altwaystohe...         4\n",
      "3  is the hp and the cursed child book up for res...         3\n",
      "4    3rd #bihday to my amazing, hilarious #nephew...         2\n"
     ]
    }
   ],
   "source": [
    "hash_tags(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of numerics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we calculated the number of words, we can also calculate the number of numerics which are present in the tweets. It is a useful feature that should be run while doing similar exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_numerics(df):\n",
    "    df['numerics'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "    print(df[['tweet','numerics']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  numerics\n",
      "0   @user when a father is dysfunctional and is s...         0\n",
      "1  @user @user thanks for #lyft credit i can't us...         0\n",
      "2                                bihday your majesty         0\n",
      "3  #model   i love u take with u all the time in ...         0\n",
      "4             factsguide: society now    #motivation         0\n"
     ]
    }
   ],
   "source": [
    "num_numerics(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  numerics\n",
      "0  #studiolife #aislife #requires #passion #dedic...         0\n",
      "1   @user #white #supremacists want everyone to s...         0\n",
      "2  safe ways to heal your #acne!!    #altwaystohe...         0\n",
      "3  is the hp and the cursed child book up for res...         0\n",
      "4    3rd #bihday to my amazing, hilarious #nephew...         0\n"
     ]
    }
   ],
   "source": [
    "num_numerics(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Uppercase words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_uppercase(df):\n",
    "    df['upper_case'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "    print(df[['tweet','upper_case']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  upper_case\n",
      "0   @user when a father is dysfunctional and is s...           0\n",
      "1  @user @user thanks for #lyft credit i can't us...           0\n",
      "2                                bihday your majesty           0\n",
      "3  #model   i love u take with u all the time in ...           0\n",
      "4             factsguide: society now    #motivation           0\n"
     ]
    }
   ],
   "source": [
    "num_uppercase(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  upper_case\n",
      "0  #studiolife #aislife #requires #passion #dedic...           0\n",
      "1   @user #white #supremacists want everyone to s...           0\n",
      "2  safe ways to heal your #acne!!    #altwaystohe...           0\n",
      "3  is the hp and the cursed child book up for res...           0\n",
      "4    3rd #bihday to my amazing, hilarious #nephew...           0\n"
     ]
    }
   ],
   "source": [
    "num_uppercase(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-CountVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CounterVectorization is a SciKitLearn library takes any text document and returns each unique word as a feature with the count of number of times that word occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "          'This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?',\n",
    "         ]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and this', 'document is', 'first document', 'is the', 'is this', 'second document', 'the first', 'the second', 'the third', 'third one', 'this document', 'this is', 'this the']\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashing Vectorizer converts text to a matrix of occurrences using the “hashing trick”. It converts a collection of text documents to a matrix of token occurrences.\n",
    "It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm=’l1’ or projected on the euclidean unit sphere if norm=’l2’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "corpus = [\n",
    "          'This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?',\n",
    "         ]\n",
    "vectorizer = HashingVectorizer(n_features=2**4)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Lower Casing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another pre-processing step which we will do is to transform our tweets into lower case. This avoids having multiple copies of the same words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(df):\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    print(df['tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    @user when a father is dysfunctional and is so...\n",
      "1    @user @user thanks for #lyft credit i can't us...\n",
      "2                                  bihday your majesty\n",
      "3    #model i love u take with u all the time in ur...\n",
      "4                  factsguide: society now #motivation\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "lower_case(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    #studiolife #aislife #requires #passion #dedic...\n",
      "1    @user #white #supremacists want everyone to se...\n",
      "2    safe ways to heal your #acne!! #altwaystoheal ...\n",
      "3    is the hp and the cursed child book up for res...\n",
      "4    3rd #bihday to my amazing, hilarious #nephew e...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "lower_case(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-Punctuation Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to remove punctuation as it doesn’t add any extra information while treating text data.\n",
    "\n",
    "Therefore removing all instances of it will help us reduce the size of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_removal(df):\n",
    "    df['tweet'] = df['tweet'].str.replace('[^\\w\\s]','')\n",
    "    print(df['tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    user when a father is dysfunctional and is so ...\n",
      "1    user user thanks for lyft credit i cant use ca...\n",
      "2                                  bihday your majesty\n",
      "3    model i love u take with u all the time in urð...\n",
      "4                    factsguide society now motivation\n",
      "Name: tweet, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayoub\\AppData\\Local\\Temp/ipykernel_17940/1163312214.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['tweet'] = df['tweet'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "punctuation_removal(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    studiolife aislife requires passion dedication...\n",
      "1    user white supremacists want everyone to see t...\n",
      "2    safe ways to heal your acne altwaystoheal heal...\n",
      "3    is the hp and the cursed child book up for res...\n",
      "4    3rd bihday to my amazing hilarious nephew eli ...\n",
      "Name: tweet, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayoub\\AppData\\Local\\Temp/ipykernel_17940/1163312214.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['tweet'] = df['tweet'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "punctuation_removal(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Stop Words Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed earlier, stop words (or commonly occurring words) should be removed from the text data.\n",
    "For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words_removal(df):\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    print(df['tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    user father dysfunctional selfish drags kids d...\n",
      "1    user user thanks lyft credit cant use cause do...\n",
      "2                                       bihday majesty\n",
      "3                model love u take u time urð ðððð ððð\n",
      "4                        factsguide society motivation\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stop_words_removal(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    studiolife aislife requires passion dedication...\n",
      "1    user white supremacists want everyone see new ...\n",
      "2    safe ways heal acne altwaystoheal healthy healing\n",
      "3    hp cursed child book reservations already yes ...\n",
      "4    3rd bihday amazing hilarious nephew eli ahmir ...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stop_words_removal(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-Frequent Words Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove commonly occurring words from our text data.\n",
    "\n",
    "First, let’s check the 10 most frequently occurring words in our text data then take call to remove or retain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user     17473\n",
       "love      2647\n",
       "ð         2511\n",
       "day       2199\n",
       "â         1797\n",
       "happy     1663\n",
       "amp       1582\n",
       "im        1139\n",
       "u         1136\n",
       "time      1110\n",
       "dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will remove these words as their presence will not of any use in classification of our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = list(freq.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_words_removal(df):    \n",
    "    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "    print(df['tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    father dysfunctional selfish drags kids dysfun...\n",
      "1    thanks lyft credit cant use cause dont offer w...\n",
      "2                                       bihday majesty\n",
      "3                              model take urð ðððð ððð\n",
      "4                        factsguide society motivation\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "frequent_words_removal(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    studiolife aislife requires passion dedication...\n",
      "1    white supremacists want everyone see new birds...\n",
      "2    safe ways heal acne altwaystoheal healthy healing\n",
      "3    hp cursed child book reservations already yes ...\n",
      "4    3rd bihday amazing hilarious nephew eli ahmir ...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "frequent_words_removal(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-Rare Words Removal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will remove rarely occurring words from the text.\n",
    "\n",
    "Because they’re so rare, the association between them and other words is dominated by noise.\n",
    "\n",
    "We can replace rare words with a more general form and then this will have higher counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hisstuff        1\n",
       "noze            1\n",
       "alanjohnson     1\n",
       "labourins       1\n",
       "pollutes        1\n",
       "contaminates    1\n",
       "surabhirao      1\n",
       "1monthtogo      1\n",
       "jessaraeep      1\n",
       "chisolm         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[-10:]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = list(freq.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rare_words_removal(df):\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "    print(df['tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    father dysfunctional selfish drags kids dysfun...\n",
      "1    thanks lyft credit cant use cause dont offer w...\n",
      "2                                       bihday majesty\n",
      "3                              model take urð ðððð ððð\n",
      "4                        factsguide society motivation\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "rare_words_removal(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    studiolife aislife requires passion dedication...\n",
      "1    white supremacists want everyone see new birds...\n",
      "2    safe ways heal acne altwaystoheal healthy healing\n",
      "3    hp cursed child book reservations already yes ...\n",
      "4    3rd bihday amazing hilarious nephew eli ahmir ...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "rare_words_removal(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-Spelling Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now tweets can be filled with plethora of spelling mistakes. Our task is to rectify these spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correction(df):\n",
    "    return df['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    father dysfunctional selfish drags kiss dysfun...\n",
       "1    thanks left credit can use cause dont offer wh...\n",
       "2                                       midday majesty\n",
       "3                               model take or ðððð ððð\n",
       "4                        factsguide society motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_correction(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    studiolife dislike requires passion education ...\n",
       "1    white supremacists want everyone see new birds...\n",
       "2    safe ways heal acne altwaystoheal healthy healing\n",
       "3    he cursed child book reservations already yes ...\n",
       "4    rd midday amazing hilarious nephew epi their u...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_correction(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization refers to dividing the text into a sequence of words or sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(df):\n",
    "    return TextBlob(df['tweet'][1]).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['thanks', 'lyft', 'credit', 'cant', 'use', 'cause', 'dont', 'offer', 'wheelchair', 'vans', 'pdx', 'disapointed', 'getthanked'])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['white', 'supremacists', 'want', 'everyone', 'see', 'new', 'birdsâ', 'movie', 'hereâs'])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach.\n",
    "\n",
    "So, stemming takes a word and refers it back to its base or root form. Stems, Stemming, Stemmed and Stemtization are all based on the single word stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(df):\n",
    "    return df['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        father dysfunct selfish drag kid dysfunct run\n",
       "1    thank lyft credit cant use caus dont offer whe...\n",
       "2                                       bihday majesti\n",
       "3                              model take urð ðððð ððð\n",
       "4                              factsguid societi motiv\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    studiolif aislif requir passion dedic willpow ...\n",
       "1    white supremacist want everyon see new birdsâ ...\n",
       "2            safe way heal acn altwaystoh healthi heal\n",
       "3    hp curs child book reserv alreadi ye ððð harry...\n",
       "4    3rd bihday amaz hilari nephew eli ahmir uncl d...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11-Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(df):\n",
    "    df['tweet'] = df['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "    print(df['tweet'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    father dysfunctional selfish drag kid dysfunct...\n",
      "1    thanks lyft credit cant use cause dont offer w...\n",
      "2                                       bihday majesty\n",
      "3                              model take urð ðððð ððð\n",
      "4                        factsguide society motivation\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "lemmatization(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    studiolife aislife requires passion dedication...\n",
      "1    white supremacist want everyone see new birdsâ...\n",
      "2     safe way heal acne altwaystoheal healthy healing\n",
      "3    hp cursed child book reservation already yes ð...\n",
      "4    3rd bihday amazing hilarious nephew eli ahmir ...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "lemmatization(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we have covered all the basic pre-processing steps which are helpful in order to clean our data.\n",
    "\n",
    "Now,we will move on and focus on advanced text-processing or NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams are the combination of multiple words used together. Ngrams with N=1 are called unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combination_of_words(df):\n",
    "    return (TextBlob(df['tweet'][0]).ngrams(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['father', 'dysfunctional']),\n",
       " WordList(['dysfunctional', 'selfish']),\n",
       " WordList(['selfish', 'drag']),\n",
       " WordList(['drag', 'kid']),\n",
       " WordList(['kid', 'dysfunction']),\n",
       " WordList(['dysfunction', 'run'])]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combination_of_words(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['studiolife', 'aislife']),\n",
       " WordList(['aislife', 'requires']),\n",
       " WordList(['requires', 'passion']),\n",
       " WordList(['passion', 'dedication']),\n",
       " WordList(['dedication', 'willpower']),\n",
       " WordList(['willpower', 'find']),\n",
       " WordList(['find', 'newmaterialsâ'])]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combination_of_words(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Term Frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(df):\n",
    "    tf1 = (df['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "    tf1.columns = ['words','tf']\n",
    "    return tf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thanks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lyft</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>credit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>use</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    words  tf\n",
       "0  thanks   1\n",
       "1    lyft   1\n",
       "2  credit   1\n",
       "3    cant   1\n",
       "4     use   1"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_frequency(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>supremacist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>want</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>everyone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>see</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words  tf\n",
       "0        white   1\n",
       "1  supremacist   1\n",
       "2         want   1\n",
       "3     everyone   1\n",
       "4          see   1"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_frequency(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Inverse Document Frequency (IDF) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind inverse document frequency (IDF) is that a word is not of much use to us if it’s appearing in all the documents.\n",
    "\n",
    "Therefore, the IDF of each word is the log of the ratio of the total number of rows to the number of rows in which that word is present.\n",
    "\n",
    "IDF can be calculated as follows -\n",
    "\n",
    "IDF = log(N/n),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thanks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lyft</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>credit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>use</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    words  tf\n",
       "0  thanks   1\n",
       "1    lyft   1\n",
       "2  credit   1\n",
       "3    cant   1\n",
       "4     use   1"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1 = (train['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']\n",
    "tf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>white</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>supremacist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>want</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>everyone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>see</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words  tf\n",
       "0        white   1\n",
       "1  supremacist   1\n",
       "2         want   1\n",
       "3     everyone   1\n",
       "4          see   1"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf2 = (test['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf2.columns = ['words','tf']\n",
    "tf2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-Term Frequency – Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 = (train['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,word in enumerate(tf1['words']):\n",
    "    tf1.loc[i, 'idf'] = np.log(train.shape[0]/(len(train[train['tweet'].str.contains(word)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thanks</td>\n",
       "      <td>1</td>\n",
       "      <td>4.597751</td>\n",
       "      <td>4.597751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lyft</td>\n",
       "      <td>1</td>\n",
       "      <td>8.762865</td>\n",
       "      <td>8.762865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>credit</td>\n",
       "      <td>1</td>\n",
       "      <td>7.327781</td>\n",
       "      <td>7.327781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cant</td>\n",
       "      <td>1</td>\n",
       "      <td>3.538194</td>\n",
       "      <td>3.538194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>use</td>\n",
       "      <td>1</td>\n",
       "      <td>3.552287</td>\n",
       "      <td>3.552287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cause</td>\n",
       "      <td>1</td>\n",
       "      <td>5.690172</td>\n",
       "      <td>5.690172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dont</td>\n",
       "      <td>1</td>\n",
       "      <td>3.745585</td>\n",
       "      <td>3.745585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>offer</td>\n",
       "      <td>1</td>\n",
       "      <td>6.522155</td>\n",
       "      <td>6.522155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wheelchair</td>\n",
       "      <td>1</td>\n",
       "      <td>9.273691</td>\n",
       "      <td>9.273691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>van</td>\n",
       "      <td>1</td>\n",
       "      <td>5.236505</td>\n",
       "      <td>5.236505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pdx</td>\n",
       "      <td>1</td>\n",
       "      <td>8.762865</td>\n",
       "      <td>8.762865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>disapointed</td>\n",
       "      <td>1</td>\n",
       "      <td>10.372303</td>\n",
       "      <td>10.372303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>getthanked</td>\n",
       "      <td>1</td>\n",
       "      <td>9.679156</td>\n",
       "      <td>9.679156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          words  tf        idf      tfidf\n",
       "0        thanks   1   4.597751   4.597751\n",
       "1          lyft   1   8.762865   8.762865\n",
       "2        credit   1   7.327781   7.327781\n",
       "3          cant   1   3.538194   3.538194\n",
       "4           use   1   3.552287   3.552287\n",
       "5         cause   1   5.690172   5.690172\n",
       "6          dont   1   3.745585   3.745585\n",
       "7         offer   1   6.522155   6.522155\n",
       "8    wheelchair   1   9.273691   9.273691\n",
       "9           van   1   5.236505   5.236505\n",
       "10          pdx   1   8.762865   8.762865\n",
       "11  disapointed   1  10.372303  10.372303\n",
       "12   getthanked   1   9.679156   9.679156"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<31962x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 114039 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    " stop_words= 'english',ngram_range=(1,1))\n",
    "train_vect = tfidf.fit_transform(train['tweet'])\n",
    "train_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words (BoW) refers to the representation of text which describes the presence of words within the text data. The intuition behind this is that two similar text fields will contain similar kind of words, and will therefore have a similar bag of words. Further, that from the text alone we can learn something about the meaning of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<31962x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 128376 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(train['tweet'])\n",
    "train_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come to our problem which was to detect the sentiment of the tweet. So, before applying any ML/DL models (which can have a separate feature detecting the sentiment using the textblob library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_subjectivity(df):\n",
    "    return df['tweet'][:5].apply(lambda x: TextBlob(x).sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    (-0.3, 0.5354166666666667)\n",
       "1                    (0.2, 0.2)\n",
       "2                    (0.0, 0.0)\n",
       "3                    (0.0, 0.0)\n",
       "4                    (0.0, 0.0)\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_subjectivity(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    (0.0, 0.0)\n",
       "1    (0.06818181818181818, 0.22727272727272727)\n",
       "2                                    (0.5, 0.5)\n",
       "3                                    (0.5, 1.0)\n",
       "4                                  (0.4, 0.625)\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_subjectivity(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can can see that it returns a tuple representing polarity and subjectivity of each tweet. Here, we only extract polarity as it indicates the sentiment as value nearer to 1 means a positive sentiment and values nearer to -1 means a negative sentiment. This can also work as a feature for building a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(df):\n",
    "    df['sentiment'] = df['tweet'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "    return df[['tweet','sentiment']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>father dysfunctional selfish drag kid dysfunct...</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thanks lyft credit cant use cause dont offer w...</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bihday majesty</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model take urð ðððð ððð</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factsguide society motivation</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment\n",
       "0  father dysfunctional selfish drag kid dysfunct...       -0.3\n",
       "1  thanks lyft credit cant use cause dont offer w...        0.2\n",
       "2                                     bihday majesty        0.0\n",
       "3                            model take urð ðððð ððð        0.0\n",
       "4                      factsguide society motivation        0.0"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>studiolife aislife requires passion dedication...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white supremacist want everyone see new birdsâ...</td>\n",
       "      <td>0.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>safe way heal acne altwaystoheal healthy healing</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hp cursed child book reservation already yes ð...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3rd bihday amazing hilarious nephew eli ahmir ...</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment\n",
       "0  studiolife aislife requires passion dedication...   0.000000\n",
       "1  white supremacist want everyone see new birdsâ...   0.068182\n",
       "2   safe way heal acne altwaystoheal healthy healing   0.500000\n",
       "3  hp cursed child book reservation already yes ð...   0.500000\n",
       "4  3rd bihday amazing hilarious nephew eli ahmir ...   0.400000"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8644d993601a5dea339df76618c3728cb83e7f84ace633619a8410e84acda8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
